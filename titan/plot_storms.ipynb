{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc50ac95-993f-4138-bb7f-e6ee59d8d3e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d193b09e-0721-416f-9402-59a26f57f5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef73ea26-bb41-4250-a0ff-5c028f476230",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6718bd05-2dde-4b5a-a77d-9ec77f7d4a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "#import matplotlib.dates as mdates\n",
    "#import cartopy.crs as ccrs\n",
    "#import cartopy.feature as cfeature\n",
    "#from shapely.geometry import Polygon\n",
    "\n",
    "# file = \"/Users/brenda/data/ams2025/titan/ascii/Tracks2Ascii.derecho.txt\"\n",
    "\n",
    "def ascii_to_df(file):\n",
    "    \n",
    "    #open file and extract column names\n",
    "    f = open(file)\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    \n",
    "    label_line_index = None  \n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        if 'labels' in line:\n",
    "            label_line_index = i\n",
    "            break  \n",
    "    labels = lines[label_line_index].split(\":\", 1)[1].strip().split(\",\")\n",
    "    \n",
    "    #the data lines are the ones that do not start with #\n",
    "    data_lines = [line.strip() for line in lines if not line.startswith(\"#\")]\n",
    "\n",
    "    rows = []\n",
    "    for line in data_lines:\n",
    "        parts = line.split()\n",
    "    \n",
    "        try:\n",
    "            # Try parsing the polygon count value (always right before 72 values)\n",
    "            poly_count_index = -73  # 72 floats + 1 count (the column starts with the numnber 72, which is not part of the values)\n",
    "    \n",
    "            # Parents and children may be missing\n",
    "            parent_str = parts[poly_count_index - 2]\n",
    "            if parent_str == '-':\n",
    "                parents = []\n",
    "            else:\n",
    "                # convert parent_str to list of ints\n",
    "                parents = [int(str) for str in parent_str.split(\",\")]  \n",
    "           \n",
    "            child_str = parts[poly_count_index - 1]\n",
    "            if child_str == '-':\n",
    "                children = []\n",
    "            else:\n",
    "                # convert parent_str to list of ints\n",
    "                children = [int(str) for str in child_str.split(\",\")]  \n",
    "           \n",
    "    \n",
    "            # Handle missing values marked as \"-\"\n",
    "            #parents = int(parent_str) if parent_str != '-' else np.nan\n",
    "            #children = int(child_str) if child_str != '-' else np.nan\n",
    "    \n",
    "            # Polygon values: skip the count, get the next 72 values\n",
    "            polygon_values = list(map(float, parts[poly_count_index + 1:]))\n",
    "    \n",
    "            # Fixed columns\n",
    "            fixed_cols = parts[:poly_count_index - 2]\n",
    "    \n",
    "            # Combine into one row\n",
    "            row = fixed_cols + [parents, children, polygon_values]\n",
    "            rows.append(row)\n",
    "        except Exception as e:\n",
    "            print(\"ERROR: skipping line: \", line)\n",
    "            continue\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Final columns: fixed + 3 custom ones\n",
    "    final_labels = labels[:len(rows[0]) - 3] + ['parents', 'children', 'nPolySidesPolygonRays']\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(rows, columns=final_labels)\n",
    "    \n",
    "    # Convert date and time columns to datetime\n",
    "    df['date_utc'] = pd.to_datetime(\n",
    "        df['Year'].astype(str) + '-' +\n",
    "        df['Month'].astype(str).str.zfill(2) + '-' +\n",
    "        df['Day'].astype(str).str.zfill(2) + ' ' +\n",
    "        df['Hour'].astype(str).str.zfill(2) + ':' +\n",
    "        df['Min'].astype(str).str.zfill(2) + ':' +\n",
    "        df['Sec'].astype(str).str.zfill(2),\n",
    "        format='%Y-%m-%d %H:%M:%S',\n",
    "        utc=True\n",
    "    )\n",
    "    # return df \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa6633b-0bbf-4839-9941-ae68a7b7af52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_helper_structures(df):\n",
    "\n",
    "    d = {}\n",
    "\n",
    "    def addit(x,y):\n",
    "        if x in d:\n",
    "            d[x].append(y)\n",
    "        else:\n",
    "            d[x] = [y]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # dictionary that associates storms (SimpleNum) with time step\n",
    "    storms_by_time_step = {}\n",
    "    # dictionary? dataframe? or just use the original df?  that associates storm (SimpleNum) with centroid, parents, and children\n",
    "    simple_storm_info = {}\n",
    "\n",
    "    # use df.apply(lambda x: axis=1)    \n",
    "\n",
    "#    for row in df:\n",
    "#        utc = row['date_utc']\n",
    "#        simple_num = row['SimpleNum']\n",
    "#        if utc in storms_by_time_step:\n",
    "#            storms_by_time_step[utc].append(simple_num)\n",
    "#        else:\n",
    "#            storms_by_time_step[utc] = [simple_num]\n",
    "       \n",
    "   \n",
    "    #if simple_num in simple_storm_info:\n",
    "        #simple_storm_info[ \n",
    "       \n",
    "\n",
    "    #utc1 = r['date_utc']\n",
    "    #k1 = utc1.iloc[0].isoformat()\n",
    "    df.apply(lambda r: addit(r['date_utc'].isoformat(), r['SimpleNum']), axis = 1)    \n",
    "    # df.apply(lambda r: addit(r['date_utc'], r['SimpleNum']), axis = 1)    \n",
    "\n",
    "    return d # storms_by_time_step, simple_storm_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899320e4-70ea-432d-83c5-c11f32bbfbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xy(simple_num, df):\n",
    "    # l = [(1,10),(2,20),(3,30)]\n",
    "    storm_df = df[df['SimpleNum'] == str(simple_num)][['VolCentroidX(km)','VolCentroidY(km)']]\n",
    "    if (storm_df.empty):\n",
    "        print(\"ERROR, storm not found: \", simple_num)\n",
    "        return\n",
    "    v_x = storm_df['VolCentroidX(km)'].to_list()[0]\n",
    "    v_y = storm_df['VolCentroidY(km)'].to_list()[0]\n",
    "    return (float(v_x), float(v_y))\n",
    "    \n",
    "# >>> get_xy('0',df)\n",
    "# (-200.502, -83.8344)\n",
    "\n",
    "\n",
    "# child_x, child_y = prepare_child_connections(df_complete, time_step_key, 'child')\n",
    "# storms_by_time is a dictionary of storm SimpleNums indexed by time\n",
    "# e.g.  {Timestamp('2022-05-21 14:59:49+0000', tz='UTC'): ['0', '1', '2', '3'], ...}\n",
    "def build_lineage(df_complete, storms_by_time, time_step_key, linkage='child'):\n",
    "\n",
    "\n",
    "    linkage_x = []\n",
    "    linkage_y = []\n",
    "\n",
    "    # get the list of storms for the time step\n",
    "    if time_step_key in storms_by_time:\n",
    "        storm_simpleNums = storms_by_time[time_step_key]\n",
    "        # [x] [y] double list comprehension? or just stack and then split?\n",
    "        #storm_simpleNum \n",
    "        #r = all_together.df[all_together.df[\"SimpleNum\"] == '3']\n",
    "        #vol_centroid_x = \n",
    "        [vol_centroid(s,df_complete) for s in storm_simpleNums]\n",
    "        # linkage_x, linkage_y = vol_centroid('5',df_complete)   # this works\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec143dc5-3a6d-4f13-b7a9-663b3044a144",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vol_centroid(storm_simple_num, df):\n",
    "\n",
    "    #storm_df = df[df['SimpleNum'] == storms[0]][['children','parents','VolCentroidX(km)','VolCentroidY(km)']]\n",
    "    storm_df = df[df['SimpleNum'] == storm_simple_num][['children','parents','VolCentroidX(km)','VolCentroidY(km)']]\n",
    "    print(\"storm_df: \")\n",
    "    print(storm_df)\n",
    "\n",
    "    children = storm_df['children'].to_list()[0]\n",
    "    parents = storm_df['parents'].to_list()[0]\n",
    "    v_x = storm_df['VolCentroidX(km)'].to_list()[0]\n",
    "    v_y = storm_df['VolCentroidY(km)'].to_list()[0]\n",
    "\n",
    "    # ok, this becomes recursive ... we are performing the same operation on all children, all parents, and then appending to lists\n",
    "\n",
    "    # storm_parents = df[df['SimpleNum'] == storms[0]]['parents']\n",
    "\n",
    "    # for each storm simple_num, get the centroid of the children\n",
    "    children_centroid_xy = [get_xy(s,df) for s in children]  # looks like [(vx,vy), (vx2,vy2) ...] \n",
    "\n",
    "    # for each storm simple_num, get the centroid of the parents\n",
    "    parents_centroid_xy = [get_xy(s,df) for s in parents]  # looks like [(vx,vy), (vx2,vy2) ...] \n",
    "\n",
    "    # format (storm_x, child1_x, None, storm_x, child2_x, None, ...)  same for parents and same for y\n",
    "    # where storm_x = (v_x, v_y), child1_x = (x,y), and None = (None, None)\n",
    "\n",
    "    lc = []\n",
    "    for p in children_centroid_xy:\n",
    "        #print(\"p: \", p)\n",
    "        lc.append((v_x, v_y))\n",
    "        lc.append(p)\n",
    "        lc.append((None,None))\n",
    "\n",
    "    lp = []\n",
    "    for p in parents_centroid_xy:\n",
    "        #print(\"p: \", p)\n",
    "        lp.append((v_x,v_y))\n",
    "        lp.append(p)\n",
    "        lp.append((None,None))\n",
    "\n",
    "    l = lc + lp \n",
    "    #linkage_x + get_xy()   # [v_x, p_x, None]\n",
    "    linkage_x = []\n",
    "    linkage_y = []\n",
    "    print(\"l = \", l)\n",
    "    if l:     # if not empty \n",
    "        linkage_x, linkage_y = zip(*l)\n",
    " \n",
    "    # insert parent (x & y), linkage(x & y), None, None, to make segments\n",
    "    # add the (simple_num, complex_num) \n",
    "    return storm_simple_num, linkage_x, linkage_y\n",
    "\n",
    "\n",
    "def vol_centroid_w_labels(storm_simple_num, df):\n",
    "\n",
    "    labels = [] # these are the labels for the arrows: [self, child, None, self, parent, ...]\n",
    "\n",
    "    #storm_df = df[df['SimpleNum'] == storms[0]][['children','parents','VolCentroidX(km)','VolCentroidY(km)']]\n",
    "    storm_df = df[df['SimpleNum'] == storm_simple_num][['children','parents','VolCentroidX(km)','VolCentroidY(km)']]\n",
    "    print(\"storm_df: \")\n",
    "    print(storm_df)\n",
    "\n",
    "    children = storm_df['children'].to_list()[0]\n",
    "    parents = storm_df['parents'].to_list()[0]\n",
    "    v_x = storm_df['VolCentroidX(km)'].to_list()[0]\n",
    "    v_y = storm_df['VolCentroidY(km)'].to_list()[0]\n",
    "\n",
    "    clabels = [(str(storm_simple_num), str(c), \"\")  for c in children]\n",
    "    plabels = [(str(p), str(storm_simple_num), \"\")  for p in parents]\n",
    "\n",
    "    labels = list(sum(clabels+plabels, ()))\n",
    "\n",
    "    # ok, this becomes recursive ... we are performing the same operation on all children, all parents, and then appending to lists\n",
    "\n",
    "    # storm_parents = df[df['SimpleNum'] == storms[0]]['parents']\n",
    "\n",
    "    # for each storm simple_num, get the centroid of the children\n",
    "    children_centroid_xy = [get_xy(s,df) for s in children]  # looks like [(vx,vy), (vx2,vy2) ...]\n",
    "\n",
    "    # for each storm simple_num, get the centroid of the parents\n",
    "    parents_centroid_xy = [get_xy(s,df) for s in parents]  # looks like [(vx,vy), (vx2,vy2) ...]\n",
    "\n",
    "    # format (storm_x, child1_x, None, storm_x, child2_x, None, ...)  same for parents and same for y\n",
    "    # where storm_x = (v_x, v_y), child1_x = (x,y), and None = (None, None)\n",
    "\n",
    "    lc = []\n",
    "    for p in children_centroid_xy:\n",
    "        #print(\"p: \", p)\n",
    "        lc.append((v_x, v_y))\n",
    "        lc.append(p)\n",
    "        lc.append((None,None))\n",
    "\n",
    "    lp = []\n",
    "    for p in parents_centroid_xy:\n",
    "        #print(\"p: \", p)\n",
    "        lp.append(p)\n",
    "        lp.append((v_x,v_y))\n",
    "        lp.append((None,None))\n",
    "\n",
    "    l = lc + lp\n",
    "    #linkage_x + get_xy()   # [v_x, p_x, None]\n",
    "    linkage_x = []\n",
    "    linkage_y = []\n",
    "    print(\"l = \", l)\n",
    "    if l:     # if not empty\n",
    "        linkage_x, linkage_y = zip(*l)\n",
    "\n",
    "    # insert parent (x & y), linkage(x & y), None, None, to make segments\n",
    "    # add the (simple_num, complex_num)\n",
    "    return labels, linkage_x, linkage_y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2db5aa7-0822-44d3-9106-adca598244fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib                                \n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# TODO: make a separate module/package for this ...\n",
    "# then, make it available like this ...\n",
    "# from colormapf import color_conversion_base, etc.\n",
    "# \n",
    "import matplotlib.colors as colors\n",
    "from matplotlib.colors import to_rgb\n",
    "\n",
    "def to_rgb(v):\n",
    "    return int(v*255)\n",
    "\n",
    "#\n",
    "# NOTE:  depends on color map file to convert X11 color names to hex\n",
    "#\n",
    "color_conversion_base = \".\"  # \"/Users/brenda/git/mica\"\n",
    "file_name = \"x11_colors_map.txt\"\n",
    "conversion_file = color_conversion_base + \"/\" + file_name\n",
    "x11_color_name_map = {}  # it is a dictionary\n",
    "# read the color name to hex conversion file\n",
    "f = open(conversion_file, \"r\")\n",
    "for line in f.readlines():\n",
    "    x = line.split()\n",
    "    x11_color_name_map[x[0]]=x[1]\n",
    "\n",
    "def normalize_colormap(edges, colors):\n",
    "    nsteps = int(edges[-1] - edges[0] + 1)\n",
    "    new_edges = np.linspace(edges[0], edges[-1], nsteps)\n",
    "    new_colors = []\n",
    "    for i in range(0, len(edges)-1):\n",
    "        for ii in range(int(edges[i]), int(edges[i+1])):\n",
    "            new_colors.append(colors[i])\n",
    "    return (new_edges, new_colors)\n",
    "\n",
    "# example using lrose-displays ...\n",
    "# lrose-displays file format:\n",
    "# MIN  MAX     NAME (name is ascii text or hex #rrggbb )\n",
    "# 0      5    yellow\n",
    "\n",
    "# TODO: make color_scale_base_dir  an input setting\n",
    "def fetch_lrose_displays_color_scale(color_scale_name, color_scale_base_dir=None):\n",
    "    if color_scale_base_dir == None:\n",
    "        # color_scale_base = \"/Users/brenda/git/lrose-displays/color_scales\"\n",
    "        color_scale_base = \"/usr/local/lrose/share/color_scales\" # /home/jovyan/share/lrose-nightly/share/color_scales\"\n",
    "    else:\n",
    "        color_scale_base = color_scale_base_dir\n",
    "    file_name = color_scale_name    # \"zdr_color\"    \n",
    "    color_scale_file = color_scale_base + \"/\" + file_name\n",
    "    color_names = []\n",
    "    edges = []\n",
    "    # read the color map file\n",
    "    f = open(color_scale_file, \"r\")\n",
    "    for line in f.readlines():\n",
    "        if line[0] != '#':\n",
    "            # print(line)\n",
    "            x = line.split()\n",
    "            print(x)\n",
    "            if len(x) >= 3:  # TODO sometimes the color name has multiple words!!! dark slate blue!!!\n",
    "                color_names.append(\"\".join(x[2:]).lower())\n",
    "                if len(edges) == 0:\n",
    "                    edges.append(float(x[0]))\n",
    "                edges.append(float(x[1]))\n",
    "    # add the ending edge\n",
    "    # display(color_names)\n",
    "    # display(edges)\n",
    "    # convert the X11 color names to hex\n",
    "    color_scale_hex = []\n",
    "    for cname in color_names:\n",
    "        if cname in x11_color_name_map:\n",
    "            color_scale_hex.append(x11_color_name_map[cname]) \n",
    "        else:\n",
    "            color_scale_hex.append(colors.to_hex(cname))\n",
    "\n",
    "    # convert color names to rgb\n",
    "    #rgb_color = to_rgb(\"dodgerblue\")\n",
    "    # define color map (matplotlib.colors.ListedColormap)\n",
    "    try:\n",
    "        # is this used???\n",
    "        norm = colors.BoundaryNorm(boundaries=edges, ncolors=len(color_names))\n",
    "        norm.autoscale(edges)\n",
    "    # TODO: the edges are NOT uniform the everything steps by 1 except the last goes 12 to 20\n",
    "        (zcmap, znorm) = colors.from_levels_and_colors(edges, color_scale_hex, extend='neither')\n",
    "    except ValueError as err:\n",
    "        print(\"something went wrong first: \", err)\n",
    "    print(\"edges: \")\n",
    "    print(edges)\n",
    "    print(\"colors: \", color_scale_hex)\n",
    "    return edges, color_scale_hex \n",
    "\n",
    "# function:  convert_to_go_colorscale takes edges and colors in hex\n",
    "#               and converts this information to a colormap format \n",
    "#               recognized by Dash graph objects,\n",
    "#               scatterpolar.\n",
    "# \n",
    "#                colorscale=[(0.00, \"red\"),   (0.33, \"red\"),\n",
    "#                    (0.33, \"green\"), (0.66, \"green\"),\n",
    "#                    (0.66, \"blue\"),  (1.00, \"blue\")],\n",
    "# color_scale_hex:  ['#483d8b', '#000080', '#0000ff', '#0000cd', '#87ceeb', '#006400', '#228b22', '#9acd32', '#bebebe', '#f5deb3', '#ffd700', '#ffff00', '#ff7f50', '#ffa500', '#c71585', '#ff4500', '#ff0000']\n",
    "# edges:  [-4.0, -3.0, -2.0, -1.0, 0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 20.0]\n",
    "# use for lrose-display and self-defined coloscales\n",
    "def convert_to_go_colorscale_old(edges, colors_hex):\n",
    "    colorscale=[]\n",
    "    max = edges[-1]\n",
    "    min = edges[0]\n",
    "    edge_range = np.abs(max-min)\n",
    "    print(\"max = \", max, \" min = \", min, \" edge_range = \", edge_range)\n",
    "    for i in range(len(edges)-1):\n",
    "       low = (edges[i]-min)/edge_range\n",
    "       high = (edges[i+1]-min)/edge_range\n",
    "       c = colors_hex[i]\n",
    "       colorscale.append((low, c))\n",
    "       colorscale.append((high, c))\n",
    "    return colorscale \n",
    "\n",
    "def convert_to_go_colorscale(edges, colors_hex):\n",
    "    colorscale=[]\n",
    "    max = edges[-1]\n",
    "    min = edges[0]\n",
    "    edge_range = np.abs(max-min)\n",
    "    print(\"max = \", max, \" min = \", min, \" edge_range = \", edge_range)\n",
    "    for i in range(len(edges)-1):\n",
    "       low = (edges[i]-min)/edge_range\n",
    "       high = (edges[i+1]-min)/edge_range\n",
    "       c = colors_hex[i]\n",
    "       colorscale.append((low, c))\n",
    "       colorscale.append((high, c))\n",
    "    return colorscale \n",
    "    \n",
    "    \n",
    "# example using matplotlib color maps ...\n",
    "def convert_to_go_colorscalei_matplotlib(color_scale_name):\n",
    "# example with colorblind friendly maps (reference Py-ART)\n",
    "    #plot_color_gradients(\n",
    "        #\"Colorblind Friendly\",\n",
    "        #[\"LangRainbow12\", \"HomeyerRainbow\", \"balance\", \"ChaseSpectral\", \"SpectralExtended\"],\n",
    "    #)\n",
    "    import numpy as np\n",
    "    #import matplotlib.cm as cm\n",
    "    \n",
    "    colormap_function = plt.get_cmap(color_scale_name)\n",
    "    edges =  np.linspace(0, 1, 20) # how to set the number of colors (256)?\n",
    "    colors_rgb = colormap_function(edges) # Get 256 colors from the colormap\n",
    "\n",
    "    # need list of tuples [(edge, color), ...]\n",
    "    clist = []\n",
    "    i = 0\n",
    "    for rgba in colors_rgb.tolist():\n",
    "        rgb_tuple = tuple(map(to_rgb, rgba[0:3]))\n",
    "        clist.append([edges[i], 'rgb'+str(rgb_tuple)])\n",
    "        i += 1\n",
    "\n",
    "    print(clist) # Output: (256, 4) - 256 colors with RGBA values\n",
    "    # RGBA values are \n",
    "    # rgba(red, green, blue, alpha)\n",
    "    # The alpha parameter is a number between 0.0 (fully transparent) and 1.0 (not transparent at all):\n",
    "   \n",
    "    return clist \n",
    "    # return convert_to_go_colorscale(edges, colors_hex)\n",
    "\n",
    "\n",
    "# here ...\n",
    "#>>> def to_rgb(v):\n",
    "#...     return int(v*255)\n",
    "#\n",
    "#>>> for rgba in c_rgb.tolist():\n",
    "#...         list(map(to_rgb, rgba[0:3]))\n",
    "\n",
    "#    The 'colorscale' property is a colorscale and may be\n",
    "#    specified as:\n",
    "#      - A list of colors that will be spaced evenly to create the colorscale.\n",
    "#        Many predefined colorscale lists are included in the sequential, diverging,\n",
    "#        and cyclical modules in the plotly.colors package.\n",
    "#      - A list of 2-element lists where the first element is the\n",
    "#        normalized color level value (starting at 0 and ending at 1),\n",
    "#        and the second item is a valid color string.\n",
    "#        (e.g. [[0, 'green'], [0.5, 'red'], [1.0, 'rgb(0, 0, 255)']])\n",
    "#      - One of the following named colorscales:\n",
    "#            ['aggrnyl', 'agsunset', 'algae', 'amp', 'armyrose', 'balance',\n",
    "#             'blackbody', 'bluered', 'blues', 'blugrn', 'bluyl', 'brbg',\n",
    "#             'brwnyl', 'bugn', 'bupu', 'burg', 'burgyl', 'cividis', 'curl',\n",
    "#             'darkmint', 'deep', 'delta', 'dense', 'earth', 'edge', 'electric',\n",
    "#             'emrld', 'fall', 'geyser', 'gnbu', 'gray', 'greens', 'greys',\n",
    "#             'haline', 'hot', 'hsv', 'ice', 'icefire', 'inferno', 'jet',\n",
    "#             'magenta', 'magma', 'matter', 'mint', 'mrybm', 'mygbm', 'oranges',\n",
    "#             'orrd', 'oryel', 'oxy', 'peach', 'phase', 'picnic', 'pinkyl',\n",
    "#             'piyg', 'plasma', 'plotly3', 'portland', 'prgn', 'pubu', 'pubugn',\n",
    "#             'puor', 'purd', 'purp', 'purples', 'purpor', 'rainbow', 'rdbu',\n",
    "#             'rdgy', 'rdpu', 'rdylbu', 'rdylgn', 'redor', 'reds', 'solar',\n",
    "#             'spectral', 'speed', 'sunset', 'sunsetdark', 'teal', 'tealgrn',\n",
    "#             'tealrose', 'tempo', 'temps', 'thermal', 'tropic', 'turbid',\n",
    "#             'turbo', 'twilight', 'viridis', 'ylgn', 'ylgnbu', 'ylorbr',\n",
    "#             'ylorrd'].\n",
    "#        Appending '_r' to a named colorscale reverses it.\n",
    "#\n",
    "\n",
    "\n",
    "# main entry point here ...\n",
    "def fetch(color_map_name):\n",
    "    # is it a matplotlib name?\n",
    "    \n",
    "    if color_map_name in plt.colormaps():\n",
    "        print(\"matplotlib knows the color scale\")\n",
    "        return convert_to_go_colorscalei_matplotlib(color_map_name)\n",
    "    # is it an lrose-display name?\n",
    "    # if ???\n",
    "    else:\n",
    "        print(\"must be an lrose-display color scale\")\n",
    "        #fetch_lrose_displays_color_scale(color_map_name)\n",
    "        edges, color_scale_hex = fetch_lrose_displays_color_scale(color_map_name)\n",
    "    # (edges_norm, colors_norm) = colormap_fetch.normalize_colormap(edges, color_scale_hex)\n",
    "    # cmap = colors.ListedColormap(colors_norm) # (color_scale_hex)\n",
    "        # print(\"color_scale_hex: \", color_scale_hex)\n",
    "        print(\"edges: \", edges)\n",
    "        # print(\"colors_norm: \", colors_norm)\n",
    "        # print(\"edges_norm: \", edges_norm)\n",
    "        colorscale_for_go = convert_to_go_colorscale(edges, color_scale_hex)\n",
    "        print(\"color scale: \")\n",
    "        print(colorscale_for_go)\n",
    "        colorscale_ticks = list(map(int, edges))\n",
    "        return colorscale_ticks, colorscale_for_go\n",
    "\n",
    "    # otherwise, return error\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bff9fd-bd5d-443d-9f44-c60c0b241bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import plotly\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import xradar as xd\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import datetime\n",
    "#import bhs\n",
    "#import build_lineage\n",
    "#import colormap_fetch\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "def file_list(directory_path):\n",
    "    files = []\n",
    "    try:\n",
    "        # Get all entries (files and directories) in the path\n",
    "        entries = os.listdir(directory_path)\n",
    "        for entry in entries:\n",
    "            full_path = os.path.join(directory_path, entry)\n",
    "            # Check if the entry is a file\n",
    "            if os.path.isfile(full_path):\n",
    "                files.append(entry)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Directory not found at '{directory_path}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    return files    \n",
    "\n",
    "# read cartesian radar data\n",
    "\n",
    "#filename = \"/Users/brenda/data/ams2025/radar/cart/qc/KingCity/20220521/ncf_20220521_173550.nc\"\n",
    "\n",
    "# KingCity goes with derecho\n",
    "path = \"/Users/brenda/data/ams2025/radar/cart/qc/KingCity/20220521\"\n",
    "\n",
    "z_step = 10\n",
    "\n",
    "def get_file_date_time(file_name):\n",
    "    return file_name[4:19]\n",
    "\n",
    "# df is a dataframe from TITAN ascii file that contains storm polygons\n",
    "# df['utc', 'polygon_x', 'polygon_y']\n",
    "def plot_with_timefile_slider(z_step, path, df, df_complete):\n",
    "\n",
    "\n",
    "    print(df.keys())\n",
    "\n",
    "    filenames = sorted(file_list(path))\n",
    "    print(\"found these data files:\")\n",
    "    print(filenames)\n",
    "   \n",
    " \n",
    "    # associate polygons and radar data files by date & time \n",
    "    # 'ncf_20220521_145949.nc'   compare to 2022-05-21 14:59:49+00:00\n",
    "    # create a parallel list of filenames with empty lists to keep the \n",
    "    #     row index of associated polygons\n",
    "    # use the filename times as the keys, and then add the polygon row idx to the list\n",
    "    associated = {get_file_date_time(f): []  for f in filenames}\n",
    "    print(associated)\n",
    "\n",
    "    df_sorted = df.sort_values(by=['utc'])\n",
    "\n",
    "    #file_date_time = get_file_date_time(filenames[file_idx])\n",
    "    #print(file_date_time)\n",
    "\n",
    "    for idx, row in df_sorted.iterrows():\n",
    "        poly_date_time = row['utc'].strftime('%Y%m%d_%H%M%S')\n",
    "        if poly_date_time in associated:\n",
    "            associated[poly_date_time].append(idx)\n",
    "        else:\n",
    "            print(\"missing data file: \", poly_date_time)\n",
    "                \n",
    "    print(associated)        \n",
    "\n",
    "    # get the storms for each time step\n",
    "    sbts = build_helper_structures(df_complete)\n",
    "    \n",
    "    # Create figure\n",
    "    fig = go.Figure()\n",
    "    fig = make_subplots(rows=1, cols=1, shared_xaxes='all', shared_yaxes='all')\n",
    "    \n",
    "    # Add traces, one for each slider step\n",
    "    #     storm traces are within each tenth (0-9 are for first time step\n",
    "    #        10 - 19 are all traces for the second file / time step   \n",
    "    # \n",
    "  \n",
    "    colorscale_ticks, colorscale_for_go = fetch(\"dbz_color\")\n",
    "    colorscale_labels = list(map(str, colorscale_ticks))  # make the ticks into labels for the color bar\n",
    "    # colorscale_labels = [\"-20\",\"5\",\"10\",\"20\",\"30\",\"35\",\"36\",\"39\",\"42\",\"45\",\"48\",\"51\",\"54\",\"57\",\"60\",\"65\",\"70\"]\n",
    "    # colorscale_ticks = [-20,5,10,20,30,35,36,39,42,45,48,51,54,57,60,65,70]\n",
    " \n",
    "    trace_count = 0\n",
    "    map_trace_indexes = []\n",
    "\n",
    "    for step in range(0, len(filenames)):\n",
    "        file = filenames[step]\n",
    "        file_path = os.path.join(path, file) \n",
    "        print(\"trying to read data file:\", file_path)\n",
    "        ds = xr.open_dataset(file_path)\n",
    "        x = ds.x0.data\n",
    "        y = ds.y0.data\n",
    "        #print(ds.z0.data[10])\n",
    "        # time_fig = make_subplots(rows=1, cols=1, \n",
    "            # shared_xaxes='all', shared_yaxes='all') # [[{\"secondary_y\": True}]])\n",
    "        #fig.add_trace(go.Scatter(x=[0,100,200,0], y=[0,200,0,0]))\n",
    "        file_date_time = get_file_date_time(file)\n",
    "        include_polygons = False\n",
    "        if include_polygons:\n",
    "            assoc_polys = associated[file_date_time]\n",
    "            for p_idx in assoc_polys:\n",
    "                # p_idx = assoc_polys[0]\n",
    "                # make a list of concatenated polygons, separated by blank/plug\n",
    "                fig.add_trace(go.Scatter(x=df['polygon_x'][p_idx], y=df['polygon_y'][p_idx]))\n",
    "                trace_count += 1\n",
    "\n",
    "        min = -20\n",
    "        max = 80\n",
    "        z_made_up = np.full_like(ds.DBZ.data[0,z_step], 37)\n",
    "\n",
    "        z_max = np.max(ds.DBZ[0], axis=0, keepdims=True)\n",
    "        z = np.nan_to_num(z_max[0].data, nan=-20)\n",
    "\n",
    "        # z_mean = np.mean(ds.DBZ[0], axis=0, keepdims=True)\n",
    "        # z = np.nan_to_num(z_mean[0].data, nan=-20)\n",
    "  \n",
    "        #z_normalized = [(n-min)/(max-min) for n in np.nan_to_num(ds.DBZ.data[0,z_step], nan=-32)]\n",
    "        fig.add_trace(\n",
    "            go.Heatmap(\n",
    "                visible=False,\n",
    "                x=x, y=y,\n",
    "                #z=z_made_up, # z needs to be between zmin and zmax\n",
    "                # z=np.nan_to_num(ds.DBZ.data[0,z_step], nan=-20),   # z needs to be between (zmin,zmax) interval !!! \n",
    "                # z_step of 4 in roughly the median height of the storm tracks.\n",
    "                # z=np.nan_to_num(ds.DBZ.data[0,4], nan=-20),   # z needs to be between (zmin,zmax) interval !!! \n",
    "                z = z,\n",
    "                #z=z_normalized,\n",
    "                type='heatmap', \n",
    "                colorscale=colorscale_for_go,\n",
    "                #colorscale=[\n",
    "                #    [0,   'rgb(0,0,255)'],\n",
    "                #    [0.5, 'rgb(255,0,255)'],\n",
    "                #    [1,   'rgb(0,255,0)'],\n",
    "                #    ],\n",
    "                #line=dict(color=\"#00CED1\", width=6),\n",
    "                # create a discrete colorscale by setting the same reference point twice in a row\n",
    "                colorbar=dict(\n",
    "                    title='dBZ',\n",
    "                    # lineposition=\"through\",\n",
    "                    tickvals=colorscale_ticks, # [0,40,80],  # in data coordinates\n",
    "                    ticktext=colorscale_labels,\n",
    "                    # lenmode=\"pixels\", len=100,\n",
    "                ),\n",
    "                name=\"v = \" + str(step),\n",
    "                zmin=-20, zmax=80,\n",
    "                ))\n",
    "\n",
    "        trace_count += 1\n",
    "\n",
    "        time_step = datetime(int(file[4:8]),int(file[8:10]),int(file[10:12]),\n",
    "            int(file[13:15]),int(file[15:17]),int(file[17:19]),tzinfo=timezone.utc)\n",
    "        # >>> fns2.isoformat()\n",
    "        # '2022-05-21T15:05:50+00:00'\n",
    "\n",
    "        sbts_key = time_step.isoformat()\n",
    "        # '2011-11-04T00:05:23'   storms by time step (sbts) keys are this format\n",
    "        storm_simple_nums_str = sbts[sbts_key]\n",
    "        # storm_simple_nums = list(map(int, storm_simple_nums_str))\n",
    "        storm_simple_nums = storm_simple_nums_str\n",
    "        print(\"sbts_key: \", sbts_key, \" storm_simple_nums: \", storm_simple_nums)\n",
    "        \n",
    "        #all_storms_t1 = [build_lineage.vol_centroid(s,df_complete) for s in storm_simple_nums]\n",
    "        all_storms_t1 = [vol_centroid_w_labels(s,df_complete) for s in storm_simple_nums]\n",
    "\n",
    "        # add the storm tracks as arrows for this time step\n",
    "        # fig.add_trace(go.Scatter(x=df['VolCentroidX(km)'], y=df['VolCentroidY(km)'],mode='lines+markers'))\n",
    "        # df[(df[\"SimpleNum\"] == 3)] \n",
    "        # use keyword None to break parents and children into segments\n",
    "        # format of coordinates: x|y=[current, child, None, current, child, None ...]\n",
    "        #child_x, child_y = prepare_child_connections(df_complete, time_step_key, 'child')\n",
    "        #parent_x, parent_y = prepare_parent_connections(df_complete, time_step_key, 'parent')\n",
    "        #for (simple_num_s, xs, ys) in all_storms_t1:\n",
    "        for (labels, xs, ys) in all_storms_t1:\n",
    "            if len(labels) > 0:\n",
    "                name = labels[0]\n",
    "            else:\n",
    "                name = 'empty'\n",
    "            # fig.add_trace(go.Scatter(x=[-100,100,None,-100,100], y=[-100,100,None,-100,-100],\n",
    "            fig.add_trace(go.Scatter(x=xs, y=ys, \n",
    "                #text=[str(simple_num_s),\"end\",\"dummy\"], \n",
    "                name=name,\n",
    "                text=labels, \n",
    "                #text=str(simple_num_s), \n",
    "                mode=\"text+lines+markers\",\n",
    "                # textposition='top right', \n",
    "                textfont=dict(color='white', size=15),\n",
    "                marker= dict(size=15,symbol= \"arrow-bar-up\", angleref=\"previous\",\n",
    "                color=\"white\")))\n",
    "            fig.update_layout(legend=dict(\n",
    "                title_text=\"Storm Simple Number\",\n",
    "                orientation=\"h\",\n",
    "                yanchor=\"bottom\",\n",
    "                y=1.02,\n",
    "                xanchor=\"right\",\n",
    "                x=1\n",
    "            ))\n",
    "            trace_count += 1\n",
    "        map_trace_indexes.append(trace_count)\n",
    "\n",
    "        #fig.add_trace(time_fig, row=1, col=1, secondary_y=False)\n",
    "    \n",
    "    # Make 10th trace visible\n",
    "    #fig.data[1].visible = True\n",
    "    \n",
    "    # to make image square \n",
    "    fig.update_layout(yaxis_scaleanchor=\"x\")\n",
    "   \n",
    "    print(\"map indexes: \", map_trace_indexes)\n",
    " \n",
    "    # Create and add slider\n",
    "    steps = []\n",
    "    for i in range(len(fig.data)):\n",
    "        step = dict(\n",
    "            method=\"update\",\n",
    "            args=[{\"visible\": [False] * len(fig.data)},\n",
    "                  {\"title\": \"Slider switched to step: \" + str(i)}],  # layout attribute\n",
    "            label=i # filenames[i][13:19]\n",
    "            #label=filenames[i][13:19]\n",
    "        )\n",
    "        step[\"args\"][0][\"visible\"][i] = True  # Toggle i'th trace to \"visible\"\n",
    "        if i in map_trace_indexes:\n",
    "            file_num = map_trace_indexes.index(i)\n",
    "            step['label'] = filenames[file_num][13:19]\n",
    "            end_trace = i\n",
    "            # fix up the traces, there are more now, with the storm lineage\n",
    "            if file_num > 0:\n",
    "                start_polygon_trace = map_trace_indexes[file_num-1] + 1\n",
    "            else:\n",
    "                start_polygon_trace = 1\n",
    "            for j in range(start_polygon_trace,end_trace+1):\n",
    "                step[\"args\"][0][\"visible\"][j] = True  # Toggle associated polygon traces to \"visible\"\n",
    "            steps.append(step)\n",
    "    \n",
    "    sliders = [dict(\n",
    "        active=1,\n",
    "        currentvalue={\"prefix\": \"time step (HHMMSS): \"},\n",
    "        pad={\"t\": 50},\n",
    "        steps=steps\n",
    "    )]\n",
    "    \n",
    "    fig.update_layout(\n",
    "        sliders=sliders\n",
    "    )\n",
    "\n",
    "    # try installing plotly in the base environment\n",
    "    fig.show(renderer=\"jupyterlab\")  # this works!\n",
    "    # return fig\n",
    "    #fig.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7efc07c-deb7-407f-b0fe-1289603b959a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3400de1-1895-4aff-8349-801fd4bc17b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# extract the polygons from the data frame and convert to cartesian coords\n",
    "\n",
    "# Convert date and time columns to datetime\n",
    "#df['date_utc'] = pd.to_datetime(\n",
    "#    df['Year'].astype(str) + '-' +\n",
    "#    df['Month'].astype(str).str.zfill(2) + '-' +\n",
    "#    df['Day'].astype(str).str.zfill(2) + ' ' +\n",
    "#    df['Hour'].astype(str).str.zfill(2) + ':' +\n",
    "#    df['Min'].astype(str).str.zfill(2) + ':' +\n",
    "#    df['Sec'].astype(str).str.zfill(2),\n",
    "#    format='%Y-%m-%d %H:%M:%S',\n",
    "#    utc=True\n",
    "#)\n",
    "\n",
    "def get_polygons_cart(df):\n",
    "\n",
    "    utc_list = []\n",
    "    polygons_x_list = []\n",
    "    polygons_y_list = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        centroid_x = float(row['VolCentroidX(km)'])\n",
    "        centroid_y = float(row['VolCentroidY(km)'])\n",
    "        centroid_z = float(row['VolCentroidZ(km)'])\n",
    "        rays = row['nPolySidesPolygonRays']\n",
    "        \n",
    "        if not rays or len(rays) == 0:\n",
    "            continue  \n",
    "        \n",
    "        angles = np.deg2rad(np.arange(0, 360, 5))  # 72 vertices at every 5 degrees\n",
    "        rays = np.array(rays, dtype=float) #from centroid to vertex\n",
    "        \n",
    "        # Rays in km to degrees\n",
    "        ray_x = rays * np.cos(angles)\n",
    "        ray_y = rays * np.sin(angles)\n",
    "    \n",
    "        # Approximate conversion from km to degrees lat/lon\n",
    "        #lat_vertices = lat_centroid + ray_y / 111\n",
    "        #lon_vertices = lon_centroid + ray_x / (111 * np.cos(np.deg2rad(lat_centroid)))\n",
    " \n",
    "        y_vertices = centroid_y + ray_y\n",
    "        x_vertices = centroid_x + ray_x\n",
    "\n",
    "        # add the first point to the end of the list to complete the polygon\n",
    "        y_vertices = np.append(y_vertices, y_vertices[0])\n",
    "        x_vertices = np.append(x_vertices, x_vertices[0])\n",
    "    \n",
    "        ##polygon_points = list(zip(lon_vertices, lat_vertices))\n",
    "        #polygon_points = list(zip(x_vertices, y_vertices))\n",
    "        \n",
    "        #poly = Polygon(polygon_points)\n",
    "        #time_idx = np.where(timesteps == row['date_utc'])[0][0]\n",
    "        time_utc = row['date_utc']\n",
    "  \n",
    "        utc_list.append(time_utc) \n",
    "        polygons_x_list.append(x_vertices) \n",
    "        polygons_y_list.append(y_vertices) \n",
    "\n",
    "        #ax.add_geometries([poly], crs=ccrs.PlateCarree(),\n",
    "        #                  edgecolor=palette[time_idx], facecolor='none', linewidth=1)\n",
    "        \n",
    "        #ax.plot(lon_centroid, lat_centroid, marker='o',color='grey', markersize=1.5, transform=ccrs.PlateCarree())\n",
    "    \n",
    "      \n",
    "   \n",
    "    d = {'utc': utc_list, 'polygon_x': polygons_x_list, 'polygon_y': polygons_y_list}\n",
    "    df = pd.DataFrame(data=d)  \n",
    "    \n",
    "    return df  \n",
    "\n",
    "    #plt.title(\"Track Polygons for ComplexNum=0\", fontsize=16)\n",
    "    #plt.show()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff9f492-f945-4775-9349-d367147f084b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2502d14f-eb1d-44c5-b78f-251085da95ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import ascii_to_df_int\n",
    "#import plot_with_timefile_slider_f\n",
    "#import get_polygons_cart\n",
    "\n",
    "\n",
    "# file = \"/Users/brenda/data/ams2025/titan/ascii/Tracks2Ascii.derecho.txt\"\n",
    "file = \"titan/Tracks2Ascii.derecho.txt\"\n",
    "\n",
    "df = ascii_to_df(file)\n",
    "df.sort_values('date_utc')\n",
    "\n",
    "df_polys = get_polygons_cart(df)\n",
    "\n",
    "# path = \"/Users/brenda/data/ams2025/radar/cart/qc/KingCity/20220521\"\n",
    "path = \"data/20220521\"\n",
    "z_step = 10\n",
    "\n",
    "fig = plot_with_timefile_slider(z_step, path, df_polys, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ad6027-da15-475f-81ec-f43398915d30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
